# ‚öñÔ∏è Scraper de Expedientes Judiciales

Este scraper transforma la b√∫squeda y gesti√≥n de informaci√≥n judicial al automatizar la recolecci√≥n de datos sobre expedientes relacionados con residuos desde el portal [Sistema de Consulta Web del Poder Judicial de la Naci√≥n de Argentina](http://scw.pjn.gov.ar/scw/home.seam). Con esta herramienta, eliminar√°s la tediosa recolecci√≥n manual, optimizando tiempo y recursos al estructurar la informaci√≥n directamente en una base de datos MySQL lista para un an√°lisis √°gil y preciso. Ideal para quienes buscan eficiencia y fiabilidad en el manejo de grandes vol√∫menes de datos judiciales.

## ‚ú® Caracter√≠sticas Principales

- **Fuente de datos:** Extrae informaci√≥n del portal SCW PJN.
- **Criterios de b√∫squeda:**
  - Filtra por jurisdicci√≥n: "COM".
  - Palabra clave: "residuos".
- **Base de datos:**
  - Creaci√≥n autom√°tica al iniciar el proyecto.
  - Tablas organizadas para detalles, participantes, acciones, notas y recursos asociados a los expedientes.
- **Tecnolog√≠as utilizadas:**
  - Python (Selenium y PyMySQL).
  - Base de datos MySQL.

## üõ†Ô∏è Tecnolog√≠as y Herramientas

- **Lenguaje principal:** Python.
- **Librer√≠as:**
  - `selenium`: Para interactuar con la web.
  - `pymysql`: Para gestionar la base de datos MySQL.
- **Base de datos:** MySQL (configurable en `config.py`).
- **Extras:** ChromeDriver para la navegaci√≥n automatizada.

## üìã Requisitos Previos

1. **Python 3.8 instalado.**
2. **MySQL configurado** en tu m√°quina o servidor.
3. **ChromeDriver instalado** y accesible en tu PATH.

## üöß Instalaci√≥n

1. Clonar el repositorio desde GitHub:
   ```bash
   git clone https://github.com/FranciscoGiayetto/Scraper-Qanlex.git
   cd Scraper-Qanlex
   ```

2. Instalar las dependencias:
   ```bash
   pip install -r requirements.txt
   ```

3. Configurar las credenciales de la base de datos en `config.py`:
   ```python
   DATABASE_CONFIG = {
       "host": "localhost",
       "user": "tu_usuario",
       "password": "tu_contrase√±a",
       "database": "expedientes"
   }
   ```

## ‚ñ∂Ô∏è Uso

1. Aseg√∫rate de que ChromeDriver est√© configurado correctamente.
2. Ejecuta el scraper:
   ```bash
   python3 main.py
   ```
3. Los datos extra√≠dos se almacenar√°n autom√°ticamente en las tablas de la base de datos MySQL.

## üìÇ Estructura de la Base de Datos

- **Details:** Informaci√≥n general del expediente.
- **Participants:** Participantes asociados al expediente.
- **Actions:** Historial de acciones realizadas.
- **Notes:** Notas relacionadas con el expediente.
- **Resources:** Recursos asociados.

## ‚ö†Ô∏è Captchas

- Actualmente, el scraper **no resuelve autom√°ticamente los captchas** presentes en el portal SCW PJN.
- Al detectar un captcha, el script pausa la ejecuci√≥n durante **20 segundos** para que el usuario lo resuelva manualmente en la ventana del navegador.

## üí° Posibles Mejoras

Para resolver captchas autom√°ticamente, se pueden integrar servicios externos mediante sus APIs. Algunas opciones son:

1. **[2Captcha](https://2captcha.com/):** Un servicio econ√≥mico para resolver captchas simples y reCAPTCHA v2/v3.
2. **[Anti-Captcha](https://anti-captcha.com/):** Similar a 2Captcha, con soporte para una amplia variedad de captchas.
3. **[Death By Captcha](http://www.deathbycaptcha.com/):** Otra alternativa popular para resolver captchas.

Integrar cualquiera de estas opciones requerir√≠a:

1. Registrar una cuenta en el servicio seleccionado y obtener una **clave API**.
2. Modificar el script para enviar captchas a la API y manejar las respuestas de forma autom√°tica.
3. Gestionar un balance suficiente en la cuenta del servicio para operar sin interrupciones.
